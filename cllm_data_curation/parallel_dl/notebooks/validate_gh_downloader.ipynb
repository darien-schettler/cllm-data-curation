{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0557de-b62a-4b7a-8e3d-d99a847f2221",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## GH DOWNLOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8696209-acc0-4f7d-a29a-5c040932b440",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487efff2-c67e-4556-b4e2-288a8e21d554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libmagic1 is already the newest version (1:5.38-4).\n",
      "libmagic1 set to manually installed.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 22.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  zstd\n",
      "0 upgraded, 1 newly installed, 0 to remove and 3 not upgraded.\n",
      "Need to get 343 kB of archives.\n",
      "After this operation, 1592 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 zstd amd64 1.4.4+dfsg-3ubuntu0.1 [343 kB]\n",
      "Fetched 343 kB in 0s (1639 kB/s)\n",
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 69943 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.4+dfsg-3ubuntu0.1_amd64.deb ...\n",
      "Unpacking zstd (1.4.4+dfsg-3ubuntu0.1) ...\n",
      "Setting up zstd (1.4.4+dfsg-3ubuntu0.1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import magic\n",
    "except:\n",
    "    !sudo apt-get install libmagic1\n",
    "    !pip install -q python-magic\n",
    "    !pip install -q lm-dataformat\n",
    "    !sudo apt-get install zstd\n",
    "    !pip install -q zstandard # Only required for zstd in terminal\n",
    "    import magic\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "import chardet\n",
    "import argparse\n",
    "import traceback\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import lm_dataformat as lmd\n",
    "from itertools import repeat\n",
    "from tqdm.notebook import tqdm; tqdm.pandas()\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f41747-8259-4aba-b512-33d720997a00",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### INITIALIZE VARIABLES AND OBJECTS FOR LATER \n",
    "\n",
    "This probably needs to be encapsulated in the functions for RH?\n",
    "\n",
    "<ul>\n",
    "    <li><b><code>.app</code>:</b> macOS application bundle, a directory containing an application's resources and executable</li>\n",
    "    <li><b><code>.bin</code>:</b> Binary file, containing compiled code or data in a non-text format</li>\n",
    "    <li><b><code>.bmp</code>:</b> Bitmap image file, an uncompressed raster image format</li>\n",
    "    <li><b><code>.bz2</code>:</b> Bzip2 compressed file, used for file compression and decompression</li>\n",
    "    <li><b><code>.class</code>:</b> Java compiled class file, containing Java bytecodeapp: macOS application bundle, a directory containing an application's resources and executable</li>\n",
    "    <li><b><code>.bin</code>:</b> Binary file, containing compiled code or data in a non-text format</li>\n",
    "    <li><b><code>.bmp</code>:</b> Bitmap image file, an uncompressed raster image format</li>\n",
    "    <li><b><code>.bz2</code>:</b> Bzip2 compressed file, used for file compression and decompression</li>\n",
    "    <li><b><code>.class</code>:</b> Java compiled class file, containing Java bytecode</li>\n",
    "    <li><b><code>.csv</code>:</b> Comma-separated values file, a plain-text format for storing tabular data</li>\n",
    "    <li><b><code>.dat</code>:</b> Generic data file, used by various applications to store data</li>\n",
    "    <li><b><code>.db</code>:</b> Database file, used to store structured data</li>\n",
    "    <li><b><code>.dll</code>:</b> Dynamic-link library, a Windows library containing code and data used by multiple programs</li>\n",
    "    <li><b><code>.dylib</code>:</b> Dynamic library, a macOS and Linux shared library containing code and data used by multiple programs</li>\n",
    "    <li><b><code>.egg</code>:</b> Python Egg, a distribution format for Python packages</li>\n",
    "    <li><b><code>.eot</code>:</b> Embedded OpenType font, a font format used for embedding fonts in web pages</li>\n",
    "    <li><b><code>.exe</code>:</b> Executable file, a program or installer that runs on Windows</li>\n",
    "    <li><b><code>.gif</code>:</b> Graphics Interchange Format, a lossless image format supporting animation</li>\n",
    "    <li><b><code>.gitignore</code>:</b> Text file specifying files and directories to be ignored by Git version control</li>\n",
    "    <li><b><code>.glif</code>:</b> Glyph interchange format, a file format for individual glyph data in a font</li>\n",
    "    <li><b><code>.gradle</code>:</b> Gradle build script, used to configure and automate the build process of a project</li>\n",
    "    <li><b><code>.gz</code>:</b> Gzip compressed file, used for file compression and decompression</li>\n",
    "    <li><b><code>.ico</code>:</b> Icon file, used to store icons for Windows applications and files</li>\n",
    "    <li><b><code>.jar</code>:</b> Java Archive, a package file format for Java libraries and applications</li>\n",
    "    <li><b><code>.jpeg</code>:</b> Joint Photographic Experts Group, a lossy image format commonly used for photographs</li>\n",
    "    <li><b><code>.jpg</code>:</b> Joint Photographic Experts Group, a lossy image format commonly used for photographs (same as 'jpeg')</li>\n",
    "    <li><b><code>.lo</code>:</b> Compiled object file, containing machine code generated from source code</li>\n",
    "    <li><b><code>.lock</code>:</b> Lock file, used to indicate a resource is in use and prevent conflicts</li>\n",
    "    <li><b><code>.log</code>:</b> Log file, a plain-text file containing a record of events or processes</li>\n",
    "    <li><b><code>.mp3</code>:</b> MPEG Audio Layer 3, a lossy audio format widely used for music and other audio content</li>\n",
    "    <li><b><code>.mp4</code>:</b> MPEG-4 Part 14, a multimedia container format used for video, audio, and other data</li>\n",
    "    <li><b><code>.nar</code>:</b> Native Archive, a file format used for distributing native binaries and libraries</li>\n",
    "    <li><b><code>.o</code>:</b> Object file, containing compiled code generated from source code</li>\n",
    "    <li><b><code>.ogg</code>:</b> Ogg container format, used for storing multimedia content, such as audio or video</li>\n",
    "    <li><b><code>.otf</code>:</b> OpenType font, a scalable font format for computer typography</li>\n",
    "    <li><b><code>.p</code>:</b> Python byte-compiled file, containing Python bytecode (same as 'pyc')</li>\n",
    "    <li><b><code>.pdf</code>:</b> Portable Document Format, a file format used to present documents independent of software, hardware, or operating systems</li>\n",
    "    <li><b><code>.png</code>:</b> Portable Network Graphics, a lossless image format supporting transparency</li>\n",
    "    <li><b><code>.pickle</code>:</b> Pickle file, a serialized Python object stored in a binary format</li>\n",
    "    <li><b><code>.pkl</code>:</b> Pickle file, a serialized Python object stored in a binary format (same as 'pickle')</li>\n",
    "    <li><b><code>.pyc</code>:</b> Python byte-compiled file, containing Python bytecode</li>\n",
    "    <li><b><code>.pyd</code>:</b> Python dynamic module, a Windows shared library containing Python extension code</li>\n",
    "    <li><b><code>.pyo</code>:</b> Python optimized byte-compiled file, containing optimized Python bytecode</li>\n",
    "    <li><b><code>.rkt</code>:</b> Racket source code file, containing code written in the Racket programming language</li>\n",
    "    <li><b><code>.so</code>:</b> Shared object, a Linux shared library containing code and data used by multiple programs</li>\n",
    "    <li><b><code>.svg</code>:</b> Scalable Vector Graphics, an XML-based vector image format for 2D graphics with support for interactivity and animation</li>\n",
    "    <li><b><code>.tar</code>:</b> Tape archive, an archive file format used to combine multiple files into a single file</li>\n",
    "    <li><b><code>.tsv</code>:</b> Tab-separated values file, a plain-text format for storing tabular data using tabs as field delimiters</li>\n",
    "    <li><b><code>.ttf</code>:</b> TrueType font, a scalable font format for computer typography</li>\n",
    "    <li><b><code>.war</code>:</b> Web Application Resource, a Java archive file format used to package Java-based web applications</li>\n",
    "    <li><b><code>.webm</code>:</b> WebM, a multimedia container format primarily used for web video and audio</li>\n",
    "    <li><b><code>.woff</code>:</b> Web Open Font Format, a font format used for embedding fonts in web pages</li>\n",
    "    <li><b><code>.woff2</code>:</b> Web Open Font Format 2, an improved version of the WOFF font format</li>\n",
    "    <li><b><code>.xz</code>:</b> XZ compressed file, used for file compression and decompression</li>\n",
    "<li><b><code>.zip</code>:</b> ZIP archive, a popular file format for data compression and archiving</li>\n",
    "        <li><b><code>.zst</code>:</b> Zstandard compressed file, a file compression format with high compression ratios and fast decompression</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62437ddb-be49-4315-b207-b30f7366b8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... CONSTANTS:\n",
      "\tn_threads           --> 36\n",
      "\tchunk_size          --> 108\n",
      "\tlen(repo_list)      --> 221035\n",
      "\tlen(repo_chunks)    --> 2047\n",
      "\tlen(bad_extensions) --> 83\n",
      "\tlen(repo_chunks[0]) --> 108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load our repos as a list\n",
    "#    - use \"../data/demo_data.csv\" if full version is not available (1/50th)\n",
    "#        --> Obtained using # full_df.sample(frac=0.02).reset_index(drop=True)\n",
    "DATA_PATH = \"../data/demo_data.csv\"\n",
    "repo_list = pd.read_csv(DATA_PATH, usecols=[\"repo_name\",])['repo_name'].to_list()\n",
    "\n",
    "# Extension filtering\n",
    "bad_extensions = ['app', 'bin', 'bmp', 'bz2', 'class', 'csv', 'dat', 'db', 'dll', 'dylib', 'egg', 'eot', 'exe', 'gif', 'gitignore', 'glif', 'gradle', 'gz', 'ico', 'jar', 'jpeg', 'jpg', 'lo', 'lock', 'log', 'mp3', 'mp4', 'nar', 'o', 'ogg', 'otf', 'p', 'pdf', 'png', 'pickle', 'pkl', 'pyc', 'pyd', 'pyo', 'rkt', 'so', 'ss', 'svg', 'tar', 'tsv', 'ttf', 'war', 'webm', 'woff', 'woff2', 'xz', 'zip', 'zst']\n",
    "media_extensions = ['3gp', 'aac', 'aif', 'aiff', 'amr', 'au', 'avi', 'bmp', 'dng', 'flac', 'flv', 'gif', 'heic', 'heif', 'ico', 'jpeg', 'jpg', 'm4a', 'm4v', 'mid', 'midi', 'mkv', 'mov', 'mp3', 'mp4', 'mpeg', 'mpg', 'ogg', 'ogv', 'opus', 'png', 'ra', 'ram', 'rm', 'svg', 't3', 'tif', 'tiff', 'wav', 'webm', 'webp', 'wmv']\n",
    "bad_extensions = sorted(list(set(bad_extensions).union(set(media_extensions))))\n",
    "mime = magic.Magic(mime=True)\n",
    "\n",
    "# Parallel Stuff\n",
    "n_rejects = len(bad_extensions)\n",
    "n_threads = cpu_count() * 3\n",
    "assert n_threads != 0\n",
    "chunk_size = n_threads * 3\n",
    "n_total = len(repo_list)\n",
    "repo_chunks = [repo_list[i:i+chunk_size] for i in range(0, n_total, max(chunk_size, 1))]\n",
    "\n",
    "# Print Check\n",
    "print(\"\\n... CONSTANTS:\")\n",
    "print(f\"\\tn_threads           --> {n_threads}\"\\\n",
    "      f\"\\n\\tchunk_size          --> {chunk_size}\"\\\n",
    "      f\"\\n\\tlen(repo_list)      --> {len(repo_list)}\"\\\n",
    "      f\"\\n\\tlen(repo_chunks)    --> {len(repo_chunks)}\"\\\n",
    "      f\"\\n\\tlen(bad_extensions) --> {n_rejects}\"\\\n",
    "      f\"\\n\\tlen(repo_chunks[0]) --> {len(repo_chunks[0])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aee68a-3886-4651-90d9-d2ab90b6487a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### TIMEOUT FUNCTIONS\n",
    "\n",
    "I don't think I'm using these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dfd1ce6-9037-47bf-be66-c3e050c9b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeoutError(Exception): \n",
    "    pass\n",
    "\n",
    "def timeout(func, args=(), kwargs={}, timeout_duration=150, default=None):\n",
    "    # wrap any function in this wrapper to raise a TimeoutError after timeout_duration secs\n",
    "    import signal\n",
    "\n",
    "    def handler(signum, frame):\n",
    "        raise TimeoutError()\n",
    "\n",
    "    # set the timeout handler\n",
    "    signal.signal(signal.SIGALRM, handler)\n",
    "    signal.alarm(timeout_duration)\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "    except TimeoutError:\n",
    "        result = default\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f8194f-5619-4bc4-8d97-ece00971a459",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### PROCESSING FUNCTIONS\n",
    "\n",
    "Some straight from Eleuther some rewritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b310c7fc-f029-45d6-b038-f8de37411918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(f):\n",
    "    # discerns filetype with mime and reads text from file if possible\n",
    "    try:\n",
    "        if not mime.from_file(f).startswith('text'): return None # Early exist\n",
    "        \n",
    "        with open(f, 'rb') as fromfh:\n",
    "            buf = fromfh.read()\n",
    "        buf = buf.decode('UTF-8')\n",
    "        if not keep(buf): \n",
    "            return None\n",
    "        else:\n",
    "            return buf\n",
    "    \n",
    "    # bad encoding, try different encoding\n",
    "    except UnicodeDecodeError:    \n",
    "        try:\n",
    "            enc = chardet.detect(buf)\n",
    "            if enc['encoding'] is None: \n",
    "                return None\n",
    "            \n",
    "            buf = buf.decode(enc['encoding'])\n",
    "            if not keep(buf):\n",
    "                return none\n",
    "            else:\n",
    "                return buf\n",
    "        except UnicodeDecodeError:\n",
    "            return\n",
    "    except KeyboardInterrupt:\n",
    "        sys.exit()\n",
    "    except FileNotFoundError:\n",
    "        # bad symlink\n",
    "        import os.path\n",
    "        if not os.path.islink(f):\n",
    "            # something went horribly wrong!\n",
    "            ...\n",
    "\n",
    "def _process_repo(repo_data, repodir):\n",
    "    def _fcheck(f):\n",
    "        fbool = (\n",
    "            '.git' not in f and \\\n",
    "            f[0]!='.' and \\\n",
    "            'LICENSE' not in f and \\\n",
    "            'node_modules' not in f and \\\n",
    "            '.min.' not in f and \\\n",
    "            f.split('.')[-1] not in bad_extensions\n",
    "        )\n",
    "        return fbool\n",
    "    \n",
    "    out = []\n",
    "    name = repo_data\n",
    "    meta = {'repo_name': name}\n",
    "    \n",
    "    try:\n",
    "        for curdir, dirs, files in os.walk(repodir):\n",
    "            files = [os.path.join(curdir, f) for f in files if _fcheck(f)]\n",
    "            filenames = [f.replace(repodir+'/', '') for f in files]\n",
    "            extensions = []\n",
    "            for f in files:\n",
    "                try:\n",
    "                    extensions.append(mime.from_file(f))\n",
    "                except FileNotFoundError:\n",
    "                    extensions.append(\"n/a\")\n",
    "                    \n",
    "            text_outputs = []\n",
    "            for f in files:\n",
    "                try:\n",
    "                    text_outputs.append(get_content(f))\n",
    "                except TimeoutError:\n",
    "                    raise TimeoutError\n",
    "                except:\n",
    "                    text_outputs.append(None)\n",
    "                    \n",
    "            for i in range(len(files)):\n",
    "                text = text_outputs[i]\n",
    "                meta_ind = copy.deepcopy(meta)\n",
    "                if text!=None:\n",
    "                    meta_ind['file_name'] = filenames[i]\n",
    "                    meta_ind['mime_type'] = extensions[i]\n",
    "                    out += [[text, meta_ind]]\n",
    "        shutil.rmtree(repodir, ignore_errors=True)\n",
    "    except TimeoutError:\n",
    "        print(f\"Processing for {name} timed out\")\n",
    "    return out\n",
    "\n",
    "def get_file_size(len_text, b_format=\"MB\"):\n",
    "    \"\"\"calculate size of text from string assuming 1 character is 1 byte\"\"\"\n",
    "    if b_format == \"KB\":\n",
    "        return len_text / 1024\n",
    "    elif b_format == \"MB\":\n",
    "        return len_text / (1024**2) # default\n",
    "    elif b_format == \"GB\":\n",
    "        return len_text / (1024**3)\n",
    "    else:\n",
    "        return len_text # bytes\n",
    "\n",
    "def get_max_line_length(text):\n",
    "    \"\"\"calculate the maximum line length in a text file represented as a string \"\"\"\n",
    "    max_line_length = 0\n",
    "    for line in text.splitlines():\n",
    "        if len(line) > max_line_length:\n",
    "            max_line_length = len(line)\n",
    "    return max_line_length\n",
    "    \n",
    "def remove_whitespace(text):\n",
    "    return re.sub(r'\\s+', '', text)\n",
    "\n",
    "def filter_duplicates(df):\n",
    "    return df[~df[\"text\"].progress_apply(remove_whitespace).duplicated()].reset_index(drop=True)\n",
    "\n",
    "def filter_max_line_length(df, max_length=1000):\n",
    "    return df[df[\"text\"].apply(get_max_line_length)<max_length].reset_index(drop=True)\n",
    "\n",
    "def filter_max_file_size(df, max_mb=1):\n",
    "    return df[df[\"text\"].apply(get_file_size)<max_mb].reset_index(drop=True)\n",
    "\n",
    "def keep(x, max_mb=1, min_n_chars=16):\n",
    "    \"\"\" overwrite previous implementation from author and only do minimal filtering \"\"\"\n",
    "    len_x = len(x)\n",
    "    if ((get_file_size(len_x)>max_mb) or (len_x<min_n_chars)):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def process_repo_list(repo_data, clone_timeout, processing_timeout, _tmp_dir=\".tmp\"):\n",
    "    if not os.path.isdir(_tmp_dir): os.makedirs(_tmp_dir, exist_ok=True)\n",
    "    try:\n",
    "        # Get repo directory path (hidden directory with repo name)\n",
    "        repo_dir = os.path.join(_tmp_dir, repo_data.rsplit(\"/\", 1)[-1])\n",
    "        \n",
    "        # clones master branch of repos with depth 1 (most recent commit only), ignoring any terminal prompts\n",
    "        p = subprocess.Popen(\n",
    "            f'GIT_TERMINAL_PROMPT=0 git clone --depth 1 --single-branch https://github.com/{repo_data} {repo_dir}',\n",
    "            shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT\n",
    "        )\n",
    "        try:\n",
    "            p.wait(clone_timeout)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f'Git clone for {repo_data} timed out ')\n",
    "            p.kill()\n",
    "        shutil.rmtree(os.path.join(repo_dir, '.git'), ignore_errors=True)\n",
    "        \n",
    "        # extracts text files from repo and returns them as list : [[text, metadata], ... ]\n",
    "        out = _process_repo(repo_data, repo_dir) #, processing_timeout=processing_timeout)        \n",
    "    except Exception:\n",
    "        print(traceback.format_exc())\n",
    "        out = None\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387da24-a592-44fd-bf81-f23bc20754d6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e2d451-4873-40fa-997d-0b7d6930f382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c193d7d112f54b8883addf0c244c1777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git clone for crazyxi/gazes timed out \n"
     ]
    }
   ],
   "source": [
    "def do_work(repo_chunks, n_threads, commit_freq=10, archive_name='/notebooks/github_data', proc_tout=600, clon_tout=600, _tmp_dir=\".tmp\"):\n",
    "    ar = lmd.Archive(archive_name)\n",
    "    pool = Pool(n_threads)\n",
    "    pbar = tqdm(repo_chunks, total=len(repo_chunks))\n",
    "    \n",
    "    success_hist = [] \n",
    "    for count, chunk in enumerate(pbar):\n",
    "        repos_out = pool.starmap(process_repo_list, zip(chunk, repeat(clon_tout), repeat(proc_tout), repeat(_tmp_dir)))\n",
    "        \n",
    "        empty_repo_cnt, non_empty_repo_cnt = 0, 0\n",
    "        for repo in repos_out:\n",
    "            if repo!=None:\n",
    "                non_empty_repo_cnt += 1\n",
    "                for f in repo: ar.add_data(f[0], meta=f[1])\n",
    "            else:\n",
    "                empty_repo_cnt += 1\n",
    "\n",
    "        # remove any leftover files\n",
    "        subprocess.Popen(f\"rm -rfv {_tmp_dir} && mkdir {_tmp_dir}\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "        \n",
    "        # Commit at appropriate intervals\n",
    "        if count % commit_freq == 0: \n",
    "            ar.commit()\n",
    "        \n",
    "        # Success stats\n",
    "        success_hist.append((non_empty_repo_cnt/len(repos_out))*100)\n",
    "        success_rate = sum(success_hist)/len(success_hist)\n",
    "        pbar.set_postfix({\"Success Rate\": success_rate})\n",
    "    ar.commit() # final commitI\n",
    "    \n",
    "do_work(repo_chunks, n_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f107c92-7c43-44ea-86b6-6575edd168e7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### OTHER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0abfd1-a993-4864-8315-24dd2b5ba42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    \"\"\"\n",
    "    Load a jsonl file into a pandas dataframe\n",
    "    \"\"\"\n",
    "    df = pd.read_json(path, lines=True,)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_pickle(obj: object, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save an object to a pickle file.\n",
    "\n",
    "    Args:\n",
    "        obj: The object to save.\n",
    "        file_path: The path to the pickle file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_pickle(file_path: str) -> object:\n",
    "    \"\"\"\n",
    "    Load an object from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the pickle file.\n",
    "\n",
    "    Returns:\n",
    "        The loaded object.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "\n",
    "# !unzstd ./github_data/*.zst\n",
    "jsonl_files = sorted(glob(\"./github_data/*.jsonl\"), key= lambda x: (int(x.split(\"_\")[-3]), int(x.split(\"_\")[-2][4:])))\n",
    "demo_jl_df = load_jsonl(jsonl_files[0])\n",
    "\n",
    "\n",
    "print(\"\\n... FILE SIZE COMPARISON --> `JSONL.ZST` vs. `.JSONL` ...\")\n",
    "!du -sh ./github_data/*\n",
    "\n",
    "print(\"\\n... DEMO JSONL FILE LOADED AS DATAFRAME ...\\n\")\n",
    "display(demo_jl_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1aa23c-0ac3-47de-ac2f-1f9597aed543",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### FN ENCAPSULATION FOR ABOVE WITH COMMENTS/DOCS\n",
    "\n",
    "This is probably unnecessary or will need more work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f10641-b122-47e1-af0b-c1483700ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load repositories as a list\n",
    "def load_repos(file_path: str) -> List[str]:\n",
    "    \"\"\"Load repository names from a CSV file and return them as a list.\"\"\"\n",
    "    repo_list = pd.read_csv(file_path, usecols=[\"repo_name\",])['repo_name'].to_list()\n",
    "    return repo_list\n",
    "\n",
    "# Filter bad file extensions\n",
    "def filter_extensions() -> List[str]:\n",
    "    \"\"\"Return a list of bad file extensions.\"\"\"\n",
    "    bad_extensions = [\n",
    "        'app', 'bin', 'bmp', 'bz2', 'class', 'csv', 'dat', 'db', 'dll', 'dylib', 'egg', 'eot', 'exe', 'gif', 'gitignore', 'glif', 'gradle', 'gz', 'ico', 'jar', 'jpeg', 'jpg', 'lo', 'lock', 'log', 'mp3', 'mp4', 'nar', 'a', 'ogg', 'otf', 'p', 'pdf', 'png', 'pickle', 'pkl', 'pyc', 'pyd', 'pyo', 'rkt', 'so', 'ss', 'svg', 'tar', 'tsv', 'ttf', 'war', 'webm', 'woff', 'woff2', 'xz', 'zip', 'zst'\n",
    "    ]\n",
    "    return bad_extensions\n",
    "\n",
    "# Initialize MIME magic\n",
    "def init_mime() -> magic.Magic:\n",
    "    \"\"\"Initialize MIME magic for file type detection.\"\"\"\n",
    "    mime = magic.Magic(mime=True)\n",
    "    return mime\n",
    "\n",
    "# Calculate the number of threads and chunk size for parallel processing\n",
    "def calculate_parallel_params() -> tuple[int, int]:\n",
    "    \"\"\"Calculate the number of threads and chunk size for parallel processing.\"\"\"\n",
    "    n_threads = cpu_count() * 3\n",
    "    assert n_threads != 0\n",
    "    chunk_size = n_threads * 3\n",
    "    return n_threads, chunk_size\n",
    "\n",
    "# Divide a list of repositories into chunks\n",
    "def divide_repo_list(repo_list: List[str], chunk_size: int) -> List[List[str]]:\n",
    "    \"\"\"Divide a list of repositories into chunks.\"\"\"\n",
    "    n_total = len(repo_list)\n",
    "    repo_chunks = [repo_list[i:i+chunk_size] for i in range(0, n_total, max(chunk_size, 1))]\n",
    "    return repo_chunks\n",
    "\n",
    "# Print check for parallel processing constants\n",
    "def print_parallel_constants(n_threads: int, chunk_size: int, repo_list: List[str], repo_chunks: List[List[str]]):\n",
    "    \"\"\"Print parallel processing constants.\"\"\"\n",
    "    print(\"\\n... CONSTANTS:\")\n",
    "    print(f\"\\tn_threads           --> {n_threads}\"\\\n",
    "          f\"\\n\\tchunk_size          --> {chunk_size}\"\\\n",
    "          f\"\\n\\tlen(repo_list)      --> {len(repo_list)}\"\\\n",
    "          f\"\\n\\tlen(repo_chunks)    --> {len(repo_chunks)}\"\\\n",
    "          f\"\\n\\tlen(repo_chunks[0]) --> {len(repo_chunks[0])}\\n\")\n",
    "    \n",
    "# Get file content if it's a text file\n",
    "def get_content(f: str, mime: magic.Magic) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Get the content of a file if it's a text file.\n",
    "\n",
    "    Args:\n",
    "        f: File path.\n",
    "        mime: MIME magic object for detecting file type.\n",
    "\n",
    "    Returns:\n",
    "        Content of the file as a string if it's a text file, otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not mime.from_file(f).startswith('text'):\n",
    "            return None  # Early exit\n",
    "\n",
    "        with open(f, 'rb') as fromfh:\n",
    "            buf = fromfh.read()\n",
    "        buf = buf.decode('UTF-8')\n",
    "        if not keep(buf): \n",
    "            return None\n",
    "        else:\n",
    "            return buf\n",
    "\n",
    "    # bad encoding, try different encoding\n",
    "    except UnicodeDecodeError:    \n",
    "        try:\n",
    "            enc = chardet.detect(buf)\n",
    "            if enc['encoding'] is None: \n",
    "                return None\n",
    "\n",
    "            buf = buf.decode(enc['encoding'])\n",
    "            if not keep(buf):\n",
    "                return None\n",
    "            else:\n",
    "                return buf\n",
    "        except UnicodeDecodeError:\n",
    "            return None\n",
    "    except KeyboardInterrupt:\n",
    "        sys.exit()\n",
    "    except FileNotFoundError:\n",
    "        # bad symlink\n",
    "        import os.path\n",
    "        if not os.path.islink(f):\n",
    "            # something went horribly wrong!\n",
    "            return None\n",
    "        \n",
    "def _fcheck(f: str, bad_extensions: List[str]) -> bool:\n",
    "    \"\"\" Check if the file is valid based on its name and extension.\n",
    "    \n",
    "    Args:\n",
    "        f: File path.\n",
    "        bad_extensions: List of bad file extensions to exclude.\n",
    "\n",
    "    Returns:\n",
    "        True if the file is valid, otherwise False.\n",
    "    \"\"\"\n",
    "    fbool = (\n",
    "        '.git' not in f and \\\n",
    "        f[0]!='.' and \\\n",
    "        'LICENSE' not in f and \\\n",
    "        'node_modules' not in f and \\\n",
    "        '.min.' not in f and \\\n",
    "        f.split('.')[-1] not in bad_extensions\n",
    "    )\n",
    "    return fbool\n",
    "\n",
    "\n",
    "def _process_repo(repo_data: str, repodir: str, bad_extensions: List[str], mime: magic.Magic) -> List[List[Union[str, dict]]]:\n",
    "    \"\"\" Process a single repository and extract text files and metadata.\n",
    "\n",
    "    Args:\n",
    "        repo_data: Repository name.\n",
    "        repodir: Repository directory path.\n",
    "        bad_extensions: List of bad file extensions to exclude.\n",
    "        mime: MIME magic object for detecting file type.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists containing text file content and metadata.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    name = repo_data\n",
    "    meta = {'repo_name': name}\n",
    "\n",
    "    try:\n",
    "        for curdir, dirs, files in os.walk(repodir):\n",
    "            files = [os.path.join(curdir, f) for f in files if _fcheck(f, bad_extensions)]\n",
    "            filenames = [f.replace(repodir+'/', '') for f in files]\n",
    "            extensions = []\n",
    "            for f in files:\n",
    "                try:\n",
    "                    extensions.append(mime.from_file(f))\n",
    "                except FileNotFoundError:\n",
    "                    extensions.append(\"n/a\")\n",
    "\n",
    "            text_outputs = []\n",
    "            for f in files:\n",
    "                try:\n",
    "                    text_outputs.append(get_content(f, mime))\n",
    "                except TimeoutError:\n",
    "                    raise TimeoutError\n",
    "                except:\n",
    "                    text_outputs.append(None)\n",
    "\n",
    "            for i in range(len(files)):\n",
    "                text = text_outputs[i]\n",
    "                meta_ind = copy.deepcopy(meta)\n",
    "                if text!=None:\n",
    "                    meta_ind['file_name'] = filenames[i]\n",
    "                    meta_ind['mime_type'] = extensions[i]\n",
    "                    out += [[text, meta_ind]]\n",
    "        shutil.rmtree(repodir, ignore_errors=True)\n",
    "    except TimeoutError:\n",
    "        print(f\"Processing for {name} timed out\")\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
